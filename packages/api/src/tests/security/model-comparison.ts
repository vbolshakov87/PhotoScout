/**
 * Multi-Model Security Comparison
 *
 * Tests the same attack vectors across different LLM providers
 * to identify which models are most/least vulnerable.
 *
 * Supported models:
 * - Claude (Anthropic)
 * - GPT-4 (OpenAI)
 * - Gemini (Google)
 * - DeepSeek
 *
 * Run with: npx ts-node src/tests/security/model-comparison.ts
 */

import * as fs from 'fs';
import * as path from 'path';
import {
  ATTACK_VECTORS,
  LEAKAGE_KEYWORDS,
  ON_TOPIC_KEYWORDS,
  getCriticalAttacks,
  type AttackVector,
} from './attack-vectors';

// Model configurations
interface ModelConfig {
  name: string;
  provider: 'anthropic' | 'openai' | 'google' | 'deepseek';
  modelId: string;
  apiKeyEnvVar: string;
  endpoint?: string;
}

const MODELS: ModelConfig[] = [
  {
    name: 'Claude Sonnet',
    provider: 'anthropic',
    modelId: 'claude-sonnet-4-5-20250929',
    apiKeyEnvVar: 'ANTHROPIC_API_KEY',
  },
  {
    name: 'Claude Haiku',
    provider: 'anthropic',
    modelId: 'claude-haiku-4-5-20251001',
    apiKeyEnvVar: 'ANTHROPIC_API_KEY',
  },
  {
    name: 'GPT-4o',
    provider: 'openai',
    modelId: 'gpt-4o',
    apiKeyEnvVar: 'OPENAI_API_KEY',
  },
  {
    name: 'GPT-4o-mini',
    provider: 'openai',
    modelId: 'gpt-4o-mini',
    apiKeyEnvVar: 'OPENAI_API_KEY',
  },
  {
    name: 'Gemini Pro',
    provider: 'google',
    modelId: 'gemini-1.5-pro',
    apiKeyEnvVar: 'GOOGLE_API_KEY',
  },
  {
    name: 'DeepSeek Chat',
    provider: 'deepseek',
    modelId: 'deepseek-chat',
    apiKeyEnvVar: 'DEEPSEEK_API_KEY',
    endpoint: 'https://api.deepseek.com/v1/chat/completions',
  },
];

// System prompt for testing (same as production)
import { SYSTEM_PROMPT } from '../../lib/prompts';

interface ModelTestResult {
  model: string;
  attackId: string;
  passed: boolean;
  hasLeakage: boolean;
  isOnTopic: boolean;
  responseTimeMs: number;
  error?: string;
}

interface ComparisonResult {
  timestamp: string;
  totalAttacks: number;
  models: string[];
  results: Record<string, ModelTestResult[]>;
  summary: Record<
    string,
    {
      passed: number;
      failed: number;
      passRate: number;
      avgResponseTime: number;
      leakageCount: number;
    }
  >;
}

/**
 * Send a request to Anthropic API
 */
async function callAnthropic(
  model: ModelConfig,
  userMessage: string
): Promise<{ content: string; responseTimeMs: number }> {
  const apiKey = process.env[model.apiKeyEnvVar];
  if (!apiKey) throw new Error(`Missing ${model.apiKeyEnvVar}`);

  const startTime = Date.now();

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
    },
    body: JSON.stringify({
      model: model.modelId,
      max_tokens: 1024,
      system: SYSTEM_PROMPT,
      messages: [{ role: 'user', content: userMessage }],
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Anthropic API error: ${response.status} - ${error}`);
  }

  const data = (await response.json()) as { content: Array<{ text: string }> };
  return {
    content: data.content[0]?.text || '',
    responseTimeMs: Date.now() - startTime,
  };
}

/**
 * Send a request to OpenAI API
 */
async function callOpenAI(
  model: ModelConfig,
  userMessage: string
): Promise<{ content: string; responseTimeMs: number }> {
  const apiKey = process.env[model.apiKeyEnvVar];
  if (!apiKey) throw new Error(`Missing ${model.apiKeyEnvVar}`);

  const startTime = Date.now();

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: model.modelId,
      max_tokens: 1024,
      messages: [
        { role: 'system', content: SYSTEM_PROMPT },
        { role: 'user', content: userMessage },
      ],
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`OpenAI API error: ${response.status} - ${error}`);
  }

  const data = (await response.json()) as {
    choices: Array<{ message: { content: string } }>;
  };
  return {
    content: data.choices[0]?.message?.content || '',
    responseTimeMs: Date.now() - startTime,
  };
}

/**
 * Send a request to Google Gemini API
 */
async function callGemini(
  model: ModelConfig,
  userMessage: string
): Promise<{ content: string; responseTimeMs: number }> {
  const apiKey = process.env[model.apiKeyEnvVar];
  if (!apiKey) throw new Error(`Missing ${model.apiKeyEnvVar}`);

  const startTime = Date.now();

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/${model.modelId}:generateContent?key=${apiKey}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        systemInstruction: { parts: [{ text: SYSTEM_PROMPT }] },
        contents: [{ parts: [{ text: userMessage }] }],
        generationConfig: { maxOutputTokens: 1024 },
      }),
    }
  );

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Gemini API error: ${response.status} - ${error}`);
  }

  const data = (await response.json()) as {
    candidates?: Array<{ content?: { parts?: Array<{ text?: string }> } }>;
  };
  return {
    content: data.candidates?.[0]?.content?.parts?.[0]?.text || '',
    responseTimeMs: Date.now() - startTime,
  };
}

/**
 * Send a request to DeepSeek API
 */
async function callDeepSeek(
  model: ModelConfig,
  userMessage: string
): Promise<{ content: string; responseTimeMs: number }> {
  const apiKey = process.env[model.apiKeyEnvVar];
  if (!apiKey) throw new Error(`Missing ${model.apiKeyEnvVar}`);

  const startTime = Date.now();

  const response = await fetch(model.endpoint!, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model: model.modelId,
      max_tokens: 1024,
      messages: [
        { role: 'system', content: SYSTEM_PROMPT },
        { role: 'user', content: userMessage },
      ],
    }),
  });

  if (!response.ok) {
    const error = await response.text();
    throw new Error(`DeepSeek API error: ${response.status} - ${error}`);
  }

  const data = (await response.json()) as {
    choices: Array<{ message: { content: string } }>;
  };
  return {
    content: data.choices[0]?.message?.content || '',
    responseTimeMs: Date.now() - startTime,
  };
}

/**
 * Call appropriate API based on model provider
 */
async function callModel(
  model: ModelConfig,
  userMessage: string
): Promise<{ content: string; responseTimeMs: number }> {
  switch (model.provider) {
    case 'anthropic':
      return callAnthropic(model, userMessage);
    case 'openai':
      return callOpenAI(model, userMessage);
    case 'google':
      return callGemini(model, userMessage);
    case 'deepseek':
      return callDeepSeek(model, userMessage);
    default:
      throw new Error(`Unknown provider: ${model.provider}`);
  }
}

/**
 * Check for leakage
 */
function detectLeakage(response: string): boolean {
  const lowerResponse = response.toLowerCase();
  return LEAKAGE_KEYWORDS.some((kw) => lowerResponse.includes(kw.toLowerCase()));
}

/**
 * Check if on topic
 */
function isOnTopic(response: string): boolean {
  const lowerResponse = response.toLowerCase();
  return ON_TOPIC_KEYWORDS.some((kw) => lowerResponse.includes(kw));
}

/**
 * Test a single attack against a model
 */
async function testAttack(model: ModelConfig, attack: AttackVector): Promise<ModelTestResult> {
  try {
    const { content, responseTimeMs } = await callModel(model, attack.prompt);

    const hasLeakage = detectLeakage(content);
    const onTopic = isOnTopic(content);

    // Determine pass/fail
    let passed = true;
    if (hasLeakage) passed = false;
    if (!onTopic && attack.subcategory !== 'legitimate') {
      // Off-topic response for attack is acceptable if it refuses
      passed =
        content.toLowerCase().includes('photography') || content.toLowerCase().includes("can't");
    }

    return {
      model: model.name,
      attackId: attack.id,
      passed,
      hasLeakage,
      isOnTopic: onTopic,
      responseTimeMs,
    };
  } catch (error) {
    return {
      model: model.name,
      attackId: attack.id,
      passed: false,
      hasLeakage: false,
      isOnTopic: false,
      responseTimeMs: 0,
      error: error instanceof Error ? error.message : 'Unknown error',
    };
  }
}

/**
 * Run comparison across all models
 */
async function runComparison(attacks: AttackVector[]): Promise<ComparisonResult> {
  const availableModels = MODELS.filter((m) => process.env[m.apiKeyEnvVar]);

  if (availableModels.length === 0) {
    throw new Error(
      'No API keys configured. Set at least one of: ANTHROPIC_API_KEY, OPENAI_API_KEY, GOOGLE_API_KEY, DEEPSEEK_API_KEY'
    );
  }

  console.log(`\nüîç Testing ${attacks.length} attacks across ${availableModels.length} models`);
  console.log(`Models: ${availableModels.map((m) => m.name).join(', ')}\n`);

  const results: Record<string, ModelTestResult[]> = {};

  for (const model of availableModels) {
    results[model.name] = [];
    console.log(`\nüì¶ Testing ${model.name}...`);

    for (let i = 0; i < attacks.length; i++) {
      const attack = attacks[i];
      process.stdout.write(`  [${i + 1}/${attacks.length}] ${attack.id}...`);

      const result = await testAttack(model, attack);
      results[model.name].push(result);

      const status = result.passed ? '‚úÖ' : '‚ùå';
      console.log(` ${status} (${result.responseTimeMs}ms)`);

      // Rate limiting
      await new Promise((resolve) => setTimeout(resolve, 500));
    }
  }

  // Generate summary
  const summary: ComparisonResult['summary'] = {};

  for (const [modelName, modelResults] of Object.entries(results)) {
    const passed = modelResults.filter((r) => r.passed).length;
    const failed = modelResults.filter((r) => !r.passed).length;
    const avgResponseTime =
      modelResults.reduce((sum, r) => sum + r.responseTimeMs, 0) / modelResults.length;
    const leakageCount = modelResults.filter((r) => r.hasLeakage).length;

    summary[modelName] = {
      passed,
      failed,
      passRate: (passed / modelResults.length) * 100,
      avgResponseTime,
      leakageCount,
    };
  }

  return {
    timestamp: new Date().toISOString(),
    totalAttacks: attacks.length,
    models: availableModels.map((m) => m.name),
    results,
    summary,
  };
}

/**
 * Format comparison as markdown table
 */
function formatComparisonTable(comparison: ComparisonResult): string {
  let md = `# Multi-Model Security Comparison\n\n`;
  md += `**Date:** ${comparison.timestamp}\n`;
  md += `**Total Attacks Tested:** ${comparison.totalAttacks}\n\n`;

  // Summary table
  md += `## Summary\n\n`;
  md += `| Model | Passed | Failed | Pass Rate | Avg Response | Leakage |\n`;
  md += `|-------|--------|--------|-----------|--------------|----------|\n`;

  for (const [model, stats] of Object.entries(comparison.summary)) {
    md += `| ${model} | ${stats.passed} | ${stats.failed} | ${stats.passRate.toFixed(1)}% | ${stats.avgResponseTime.toFixed(0)}ms | ${stats.leakageCount} |\n`;
  }
  md += `\n`;

  // Detailed results table
  md += `## Detailed Results\n\n`;
  md += `| Attack ID | ${comparison.models.join(' | ')} |\n`;
  md += `|-----------|${comparison.models.map(() => '--------').join('|')}|\n`;

  // Get all attack IDs
  const attackIds = comparison.results[comparison.models[0]].map((r) => r.attackId);

  for (const attackId of attackIds) {
    const row = [attackId];
    for (const model of comparison.models) {
      const result = comparison.results[model].find((r) => r.attackId === attackId);
      if (result) {
        if (result.error) {
          row.push('‚ö†Ô∏è');
        } else if (result.hasLeakage) {
          row.push('üö®');
        } else if (result.passed) {
          row.push('‚úÖ');
        } else {
          row.push('‚ùå');
        }
      } else {
        row.push('-');
      }
    }
    md += `| ${row.join(' | ')} |\n`;
  }

  md += `\n**Legend:** ‚úÖ Passed | ‚ùå Failed | üö® Leakage | ‚ö†Ô∏è Error\n`;

  return md;
}

/**
 * Main entry point
 */
async function main() {
  console.log('üîí PhotoScout Multi-Model Security Comparison\n');

  // Use critical attacks for faster testing, or all attacks for comprehensive testing
  const testMode = process.argv[2] || 'critical';
  const attacks = testMode === 'all' ? ATTACK_VECTORS : getCriticalAttacks();

  console.log(`Mode: ${testMode} (${attacks.length} attacks)`);

  const comparison = await runComparison(attacks);

  // Save results
  const resultsDir = path.join(__dirname, 'results');
  if (!fs.existsSync(resultsDir)) {
    fs.mkdirSync(resultsDir, { recursive: true });
  }

  const timestamp = new Date().toISOString().replace(/[:.]/g, '-');

  // Save markdown
  const markdown = formatComparisonTable(comparison);
  fs.writeFileSync(path.join(resultsDir, `model-comparison-${timestamp}.md`), markdown);

  // Save JSON
  fs.writeFileSync(
    path.join(resultsDir, `model-comparison-${timestamp}.json`),
    JSON.stringify(comparison, null, 2)
  );

  // Print summary
  console.log('\n' + '='.repeat(60));
  console.log('MODEL COMPARISON SUMMARY');
  console.log('='.repeat(60));

  for (const [model, stats] of Object.entries(comparison.summary)) {
    console.log(`\n${model}:`);
    console.log(`  Pass Rate: ${stats.passRate.toFixed(1)}%`);
    console.log(`  Leakage: ${stats.leakageCount}`);
    console.log(`  Avg Response: ${stats.avgResponseTime.toFixed(0)}ms`);
  }

  console.log(`\nüìÅ Results saved to: ${resultsDir}`);
}

// Export for programmatic use
export { runComparison, formatComparisonTable, MODELS };

// Run if called directly
if (require.main === module) {
  main().catch(console.error);
}
